#!_PYTHON_

#################################################################################
### dynamo-fileopd ##############################################################
###
### File operations daemon that acts on the transfer and deletion tasks created
### by the Dynamo file operations manager (FOM). Tasks are listed in MySQL tables
### ("queues"). This daemon is responsible for picking up tasks from the queues
### and executing gfal2 copies or deletions, while driving the task state machine.
### Parallel operations are implemented using multiprocessing.Pool. One Pool is
### created per source-destination pair (target site) in transfers (deletions).
### Because each gfal2 operation reserves a network port, the machine must have
### sufficient number of open ports for this daemon to operate.
### Task state machine:
### Tasks arrive at the queue in 'new' state. The possible transitions are
###  new -> queued       ... When the task is added to the operation pool
###  queued -> active    ... When the task operation started
###  active -> done      ... When the task operation succeeded
###  active -> failed    ... When the task operation failed
###  new -> cancelled    ... When the task is cancelled by the FOM
###  queued -> cancelled ... When the task is cancelled by the FOM
#################################################################################

import os
import pwd
import time
import threading
import signal
import logging
import logging.handlers
import gfal2

if __name__ == '__main__':
    ## Raise the process maximums to accommodate large number of subprocs and pipes
    import resource
    resource.setrlimit(resource.RLIMIT_NPROC, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
    resource.setrlimit(resource.RLIMIT_NOFILE, (65536, 65536))

    ## Read server config (should be readable only to root)
    from dynamo.dataformat import Configuration
    from dynamo.core.serverutils import BANNER
    from dynamo.utils.log import log_exception
    from dynamo.utils.interface.mysql import MySQL
    from dynamo.utils.signaling import SignalConverter

    config_path = os.getenv('DYNAMO_SERVER_CONFIG', '/etc/dynamo/server_config.json')    
    config = Configuration(config_path)
    
    ## Set up logging (write to stderr unless path is given)
    log_level = getattr(logging, config.logging.level.upper())
    log_format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s'
    
    LOG = logging.getLogger()
    LOG.setLevel(log_level)
    if config.logging.get('path', ''):
        log_handler = logging.handlers.RotatingFileHandler(config.logging.path + '/fod.log', maxBytes = 10000000, backupCount = 100)
    else:
        log_handler = logging.StreamHandler()
    LOG.addHandler(log_handler)
    
    ## Print some nice banner before we start logging with the timestamp format
    LOG.critical(BANNER)
    
    log_handler.setFormatter(logging.Formatter(fmt = log_format))

    ## Set the effective user id to config.user
    try:
        pwnam = pwd.getpwnam(config.user)
        os.setegid(pwnam.pw_gid)
        os.seteuid(pwnam.pw_uid)
    except OSError:
        LOG.warning('Cannot switch uid to %s (%d).', config.user, pwd.getpwnam(config.user).pw_uid)

    ## File operations config
    daemon_config = config.file_operations.daemon
    
    ## Set up operational parameters
    # We want to make these parameters dynamic in the future
    # (which means we'll have to create a new table that records the site names for each batch)
    max_concurrent = daemon_config.max_parallel_links
    transfer_timeout = daemon_config.transfer_timeout
    overwrite = daemon_config.get('overwrite', False)
    x509_proxy = daemon_config.get('x509_proxy', '')
    staging_x509_proxy = daemon_config.get('staging_x509_proxy', x509_proxy)

    if 'gfal2_verbosity' in daemon_config:
        gfal2.set_verbose(getattr(gfal2.verbose_level, daemon_config.gfal2_verbosity.lower()))

    params_config = {
        'transfer_nstreams': 1,
        'transfer_timeout': transfer_timeout,
        'overwrite': overwrite
    }

    ## Pool managers
    from dynamo.fileop.daemon.manager import PoolManager
    from dynamo.fileop.daemon.transfer import TransferPoolManager
    from dynamo.fileop.daemon.delete import DeletionPoolManager, UnmanagedDeletionPoolManager
    from dynamo.fileop.daemon.stage import StagingPoolManager

    ## Set up a handle to the DB
    db = MySQL(daemon_config.db_params)
  
    ## Convert SIGTERM and SIGHUP into KeyboardInterrupt (SIGINT already is)
    PoolManager.signal_converter = SignalConverter()
    PoolManager.signal_converter._logger = LOG
    PoolManager.signal_converter.set(signal.SIGTERM)
    PoolManager.signal_converter.set(signal.SIGHUP)

    ## Collect PoolManagers
    transfer_managers = {}
    staging_managers = {}
    deletion_managers = {}
    unmanaged_deletion_managers = {}

    ## Flag to stop the managers
    stop_flag = threading.Event()

    ## Set the pool manager statics (MySQL class is multiprocess-safe)
    PoolManager.db = db
    PoolManager.stop_flag = stop_flag

    ## Pool manager getters
    def get_transfer_manager(src, dest, max_concurrent):
        try:
            return transfer_managers[(src, dest)]
        except KeyError:
            transfer_managers[(src, dest)] = TransferPoolManager(src, dest, max_concurrent, x509_proxy)
            return transfer_managers[(src, dest)]

    def get_staging_manager(src, max_concurrent):
        try:
            return staging_managers[src]
        except KeyError:
            staging_managers[src] = StagingPoolManager(src, max_concurrent, staging_x509_proxy)
            return staging_managers[src]

    def get_deletion_manager(site, max_concurrent):
        try:
            return deletion_managers[site]
        except KeyError:
            deletion_managers[site] = DeletionPoolManager(site, max_concurrent, x509_proxy)
            return deletion_managers[site]

    def get_unmanaged_deletion_manager(site, max_concurrent):
        try:
            return unmanaged_deletion_managers[site]
        except KeyError:
            unmanaged_deletion_managers[site] = UnmanagedDeletionPoolManager(site, max_concurrent, x509_proxy)
            return unmanaged_deletion_managers[site]

    ## Start loop
    try:
        # If the previous cycle ended with a crash, there may be some dangling tasks in the queued state
        sql = 'UPDATE `standalone_deletion_tasks` SET `status` = \'new\' WHERE `status` IN (\'queued\', \'active\')'
        db.query(sql)
        sql = 'UPDATE `standalone_transfer_tasks` SET `status` = \'new\' WHERE `status` IN (\'queued\', \'active\')'
        db.query(sql)

        deletion_first_wait = True
        transfer_first_wait = True

        while True:
            ## Create deletion tasks (batched by site)
            if deletion_first_wait:
                LOG.info('Creating deletion tasks.')
                deletion_first_wait = False
            else:
                LOG.debug('Creating deletion tasks.')
        
            sql = 'SELECT q.`id`, a.`file`, b.`site` FROM `standalone_deletion_tasks` AS a'
            sql += ' INNER JOIN `deletion_tasks` AS q ON q.`id` = a.`id`'
            sql += ' INNER JOIN `standalone_deletion_batches` AS b ON b.`batch_id` = q.`batch_id`'
            sql += ' WHERE a.`status` = \'new\''
            sql += ' ORDER BY b.`site`, q.`id`'
        
            _site = ''
            for tid, pfn, site in db.query(sql):
                if site != _site:
                    _site = site
                    pool_manager = get_deletion_manager(site, max_concurrent)

                pool_manager.add_task(tid, pfn)

                deletion_first_wait = True

            ## Create unmanaged deletion (empty directories and orphan files) tasks
            
            sql = 'SELECT `id`, `url`, `site` FROM `unmanaged_deletions` ORDER BY `site`'

            _site = ''
            for tid, url, site in db.query(sql):
                if site != _site:
                    _site = site
                    pool_manager = get_unmanaged_deletion_manager(site, max_concurrent)

                pool_manager.add_task(tid, url)

                deletion_first_wait = True

            ## Create transfer tasks (batched by site)
            if transfer_first_wait:
                LOG.info('Creating transfer tasks.')
                transfer_first_wait = False
            else:
                LOG.debug('Creating transfer tasks.')

            # First find batches with tape source
            batch_sql = 'SELECT `batch_id` FROM `standalone_transfer_batches` WHERE `mss_source` = 1 AND `stage_token` IS NULL'
            batch_update_sql = 'UPDATE `standalone_transfer_batches` SET `stage_token` = %s WHERE `batch_id` = %s'

            task_sql = 'SELECT a.`id`, a.`source` FROM `standalone_transfer_tasks` AS a'
            task_sql += ' INNER JOIN `transfer_tasks` AS q ON q.`id` = a.`id`'
            task_sql += ' WHERE q.`batch_id` = %s'
            task_update_sql = 'UPDATE `standalone_transfer_tasks` SET `status` = %s WHERE `id` = %s'

            if staging_x509_proxy:
                # Current installed version of gfal2 (1.9.3) does not have the ability to switch credentials based on URL
                uporig = os.getenv('X509_USER_PROXY', None)
                os.environ['X509_USER_PROXY'] = staging_x509_proxy

            for batch_id in db.query(batch_sql):
                tasks = db.query(task_sql, batch_id)
                pfns = [t[1] for t in tasks]

                # PFNs, pintime, timeout, async; I have no idea what pintime and timeout values should be
                bring_online_response = gfal_exec('bring_online', (pfns, 0, 0, True), return_value = True)

                db.query(batch_update_sql, bring_online_response[1], batch_id)

                for (tid, pfn), err in zip(tasks, bring_online_response[0]):
                    if err is None:
                        db.query(task_update_sql, 'staging', tid)
                    else:
                        db.query(task_update_sql, 'failed', tid)

            if staging_x509_proxy:
                if uporig is None:
                    os.environ.pop('X509_USER_PROXY')
                else:
                    os.environ['X509_USER_PROXY'] = uporig

            # Next poll staging tasks
            sql = 'SELECT q.`id`, a.`source`, b.`source_site`, b.`stage_token` FROM `standalone_transfer_tasks` AS a'
            sql += ' INNER JOIN `transfer_tasks` AS q ON q.`id` = a.`id`'
            sql += ' INNER JOIN `standalone_transfer_batches` AS b ON b.`batch_id` = q.`batch_id`'
            sql += ' WHERE a.`status` = \'staging\''
            sql += ' ORDER BY b.`source_site`, q.`id`'

            _site = ''
            for tid, src_pfn, ssite, token in db.query(sql):
                if ssite != _site:
                    _site = ssite
                    pool_manager = get_staging_manager(ssite, max_concurrent)

                pool_manager.add_task(tid, src_pfn, token)

            # Finally start transfers for tasks in new and staged states
            sql = 'SELECT q.`id`, a.`source`, a.`destination`, a.`checksum_algo`, a.`checksum`, b.`source_site`, b.`destination_site`'
            sql += ' FROM `standalone_transfer_tasks` AS a'
            sql += ' INNER JOIN `transfer_tasks` AS q ON q.`id` = a.`id`'
            sql += ' INNER JOIN `standalone_transfer_batches` AS b ON b.`batch_id` = q.`batch_id`'
            sql += ' WHERE (a.`status` = \'new\' AND b.`mss_source` = 0) OR a.`status` = \'staged\''
            sql += ' ORDER BY b.`source_site`, b.`destination_site`, q.`id`'
        
            _link = None
            for tid, src_pfn, dest_pfn, algo, checksum, ssite, dsite in db.query(sql):
                if (ssite, dsite) != _link:
                    _link = (ssite, dsite)
                    pool_manager = get_transfer_manager(ssite, dsite, max_concurrent)

                pconf = dict(params_config)
                if algo:
                    # Available checksum algorithms: crc32, adler32, md5
                    pconf['checksum'] = (gfal2.checksum_mode.target, algo, checksum)
        
                pool_manager.add_task(tid, src_pfn, dest_pfn, pconf)

                transfer_first_wait = True

            ## Recycle threads
            for managers in [transfer_managers, staging_managers, deletion_managers, unmanaged_deletion_managers]:
                for key, manager in managers.items():
                    if manager.ready_for_recycle():
                        LOG.info('Recycling pool manager %s', manager.name)
                        managers.pop(key)

            time.sleep(30)

    except KeyboardInterrupt:
        pass

    except:
        log_exception(LOG)

    finally:
        stop_flag.set()

        try:
            # try to clean up
            sql = 'UPDATE `standalone_deletion_tasks` SET `status` = \'new\' WHERE `status` IN (\'queued\', \'active\')'
            db.query(sql)
            sql = 'UPDATE `standalone_transfer_tasks` SET `status` = \'new\' WHERE `status` IN (\'queued\', \'active\')'
            db.query(sql)
        except:
            pass

    while True:
        # PoolManagers will terminate automatically once stop_flag is set
        all_managers = [('Transfer', transfer_managers), ('Staging', staging_managers), ('Deletion', deletion_managers), ('UnmanagedDeletion', unmanaged_deletion_managers)]
        all_clear = True

        for name, managers in all_managers:
            for key, manager in managers.items():
                if manager.ready_for_recycle():
                    managers.pop(key)

            if len(managers) != 0:
                LOG.info('%s managers still alive: %s', name, ' '.join(m.name for m in managers.itervalues()))
                all_clear = False

        if all_clear:
            break
        else:
            time.sleep(1)

    LOG.info('dynamo-fileopd terminated.')
