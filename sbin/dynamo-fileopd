#!/usr/bin/env python

import os
import sys
import pwd
import time
import threading
import signal
import multiprocessing
import logging
import logging.handlers
import tempfile
import gfal2

def transfer(src_pfn, dest_pfn, params_config):
    try:
        params = gfal2.Gfal2Context.transfer_parameters()
        # Create parent directories at the destination
        params.create_parent = True
        # Overwrite the destination if file already exists (otherwise throws an error)
        params.overwrite = True
        params.set_checksum(*params_config['checksum'])
        params.timeout = params_config['transfer_timeout'] # we probably want this to be file size dependent
    
    except Exception as exc:
        # multiprocessing pool cannot handle certain exceptions - convert to string
        raise Exception(str(exc))

    def docopy(params, src_pfn, dest_pfn):
        gfal2.creat_context().filecopy(params, src_pfn, dest_pfn)

    return gfal_exec(docopy, (params, src_pfn, dest_pfn))

def delete(pfn):
    def dodelete(pfn):
        gfal2.creat_context().unlink(pfn)

    return gfal_exec(dodelete, (pfn,))

def gfal_exec(func, args):
    start_time = 0
    finish_time = 0
    log = ''

    for attempt in xrange(5):
        # redirect stdout and stderr to a temporary file
        stream = tempfile.TemporaryFile() 
        stdout_fileno = sys.stdout.fileno()
        stderr_fileno = sys.stderr.fileno()
        stdout_copy = os.dup(stdout_fileno)
        stderr_copy = os.dup(stderr_fileno)
        os.dup2(stream.fileno(), stdout_fileno)
        os.dup2(stream.fileno(), stderr_fileno)
    
        try:
            gfal2.set_verbose(gfal2.verbose_level.verbose)

            start_time = int(time.time())

            func(*args)

            finish_time = int(time.time())
        
        except gfal2.GError as err:
            exitcode, msg = err.code, err.message

            if err.code == 70:
                continue
    
        else:
            exitcode, msg = 0, ''
    
        finally:
            sys.stdout.flush()
            sys.stderr.flush()

            stream.seek(0)
            log_tmp = stream.read().strip()

            os.dup2(stdout_copy, stdout_fileno)
            os.dup2(stderr_copy, stderr_fileno)

        # give a nice indent to each line
        log = ''.join('  %s\n' % line for line in log_tmp.split('\n'))
    
        stream.close()

        break

    # all three variables would be defined even when all attempts are exhausted
    return exitcode, start_time, finish_time, msg, log

def batch_transfer(batch_id, tasks, params_config, max_concurrent, db, stop_flag):
    LOG.info('Transfer batch %d (%d tasks)', batch_id, len(tasks))

    pool = multiprocessing.Pool(max_concurrent)
    results = []

    for tid, src_pfn, dest_pfn in tasks:
        LOG.debug('Transfer %s->%s', src_pfn, dest_pfn)
        results.append((tid, src_pfn, dest_pfn, pool.apply_async(transfer, (src_pfn, dest_pfn, params_config))))

    pool.close()

    collect_results('transfer', results, db, stop_flag)

    if stop_flag.is_set():
        pool.terminate()

    pool.join()

def batch_delete(batch_id, tasks, max_concurrent, db, stop_flag):
    LOG.info('Deletion batch %d (%d tasks)', batch_id, len(tasks))

    pool = multiprocessing.Pool(max_concurrent)
    results = []

    for tid, pfn in tasks:
        LOG.debug('Deletion %s', pfn)
        results.append((tid, pfn, pool.apply_async(delete, (pfn,))))

    pool.close()

    collect_results('deletion', results, db, stop_flag)

    if stop_flag.is_set():
        pool.terminate()

    pool.join()

def start_batch_transfer(batch_id, tasks, params_config, max_concurrent, db, stop_flag, threads):
    thread = threading.Thread(target = batch_transfer, args = (batch_id, tasks, params_config, max_concurrent, db, stop_flag), name = 'transfer_%d' % batch_id)
    thread.start()
    threads.append(thread)

def start_batch_delete(batch_id, tasks, max_concurrent, db, stop_flag, threads):
    thread = threading.Thread(target = batch_delete, args = (batch_id, tasks, max_concurrent, db, stop_flag), name = 'deletion_%d' % batch_id)
    thread.start()
    threads.append(thread)

def collect_results(optype, results, db, stop_flag):
    sql = 'UPDATE `standalone_{op}_queue` SET `status` = %s, `exitcode` = %s, `start_time` = FROM_UNIXTIME(%s), `finish_time` = FROM_UNIXTIME(%s) WHERE `id` = %s'.format(op = optype)

    delim = '--------------'

    while len(results) != 0:
        ir = 0
        while ir != len(results):
            if stop_flag.is_set():
                break

            if not results[ir][-1].ready():
                ir += 1
                continue

            result_tuple = results.pop(ir)
    
            if optype == 'transfer':
                tid, src, dest, result = result_tuple
                exitcode, start_time, finish_time, msg, log = result.get()
    
                if exitcode == 0:
                    LOG.info('Transfer success (%d seconds)\n%s -> %s\n%s\n%s%s', finish_time - start_time, src, dest, delim, log, delim)
                    status = 'done'
                else:
                    LOG.info('Transfer failure (%d seconds, code %d: %s)\n%s -> %s\n%s\n%s%s', finish_time - start_time, exitcode, msg, src, dest, delim, log, delim)
                    status = 'failed'

            else:
                tid, pfn, result = result_tuple
                exitcode, start_time, finish_time, msg, log = result.get()
    
                if exitcode == 0:
                    LOG.info('Deletion success (%d seconds)\n%s\n%s\n%s%s', finish_time - start_time, pfn, delim, log, delim)
                    status = 'done'
                else:
                    LOG.info('Deletion failure (%d seconds, code %d: %s)\n%s\n%s\n%s%s', finish_time - start_time, exitcode, msg, pfn, delim, log, delim)
                    status = 'failed'

            db.query(sql, status, exitcode, start_time, finish_time, tid)

        if stop_flag.is_set():
            break

        if ir != 0:
            time.sleep(5)


if __name__ == '__main__':
    ## Read server config (should be readable only to root)
    from dynamo.dataformat import Configuration
    import dynamo.core.server as main
    
    try:
        config_path = os.environ['DYNAMO_SERVER_CONFIG']
    except KeyError:
        config_path = '/etc/dynamo/server_config.json'
    
    config = Configuration(config_path)
    
    ## Set up logging (write to stderr unless path is given)
    log_level = getattr(logging, config.logging.level.upper())
    log_format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s'
    
    LOG = logging.getLogger()
    LOG.setLevel(log_level)
    if config.logging.get('path', ''):
        log_handler = logging.handlers.RotatingFileHandler(config.logging.path + '/fod.log', maxBytes = 10000000, backupCount = 100)
    else:
        log_handler = logging.StreamHandler()
    LOG.addHandler(log_handler)
    
    ## Print some nice banner before we start logging with the timestamp format
    LOG.critical(main.serverutils.BANNER)
    
    log_handler.setFormatter(logging.Formatter(fmt = log_format))

    from dynamo.utils.log import log_exception

    ## Set the effective user id to config.user
    try:
        pwnam = pwd.getpwnam(config.user)
        os.setegid(pwnam.pw_gid)
        os.seteuid(pwnam.pw_uid)
    except OSError:
        LOG.warning('Cannot switch uid to %s (%d).', config.user, pwd.getpwnam(config.user).pw_uid)

    ## File operations config
    fileop_config = config.file_operations
    
    ## Set up operational parameters
    # We probably want to make this link specific (which means we'll have to create a new table that records the site names for each batch)
    max_concurrent = fileop_config.daemon.max_parallel_links
    checksum_algo = fileop_config.daemon.get('checksum', '')
    transfer_timeout = fileop_config.daemon.transfer_timeout

    if 'gfal2_verbosity' in fileop_config.daemon:
        gfal2.set_verbose(getattr(gfal2.verbose_level, fileop_config.daemon.gfal2_verbosity.lower()))

    max_batches = fileop_config.daemon.max_batches

    ## Set up a handle to the DB
    from dynamo.utils.interface.mysql import MySQL

    db = MySQL(fileop_config.manager.db.db_params)
   
    ## Convert SIGTERM and SIGHUP into KeyboardInterrupt (SIGINT already is)
    from dynamo.utils.signaling import SignalConverter
    signal_converter = SignalConverter(logger = LOG)
    signal_converter.set(signal.SIGTERM)
    signal_converter.set(signal.SIGHUP)

    ## Collect threads
    threads = []

    stop_flag = threading.Event()

    ## Start loop
    try:
        while True:
            ## Create deletion tasks (batched by site)
            LOG.info('Creating deletion tasks.')
        
            sql = 'SELECT q.`batch_id`, q.`id`, a.`file` FROM `standalone_deletion_queue` AS a'
            sql += ' INNER JOIN `deletion_queue` AS q ON q.`id` = a.`id`'
            sql += ' WHERE a.`status` = \'new\''
            sql += ' ORDER BY q.`batch_id`'
        
            _batch_id = 0
            
            for batch_id, tid, lfn, pfn in db.xquery(sql):
                if batch_id != _batch_id:
                    if _batch_id != 0:
                        start_batch_delete(_batch_id, tasks, max_concurrent, db, stop_flag, threads)
        
                    _batch_id = batch_id
                    tasks = []
        
                tasks.append((tid, pfn))
        
            if _batch_id != 0:
                start_batch_delete(_batch_id, tasks, max_concurrent, db, stop_flag, threads)
        
            ## Create transfer tasks (batched by site)
            LOG.info('Creating transfer tasks.')
        
            params_config = {
                'transfer_nstreams': 1,
                'transfer_timeout': transfer_timeout
            }
        
            if checksum_algo:
                # Available checksum algorithms: crc32, adler32, md5
                params_config['checksum'] = (gfal2.checksum_mode.both, checksum_algo, '')
            else:
                params_config['checksum'] = (gfal2.checksum_mode.none, '', '')
        
            sql = 'SELECT q.`batch_id`, q.`id`, a.`source`, a.`destination` FROM `standalone_transfer_queue` AS a'
            sql += ' INNER JOIN `transfer_queue` AS q ON q.`id` = a.`id`'
            sql += ' WHERE a.`status` = \'new\''
            sql += ' ORDER BY q.`batch_id`'
        
            _batch_id = 0
            
            for batch_id, tid, source, destination in db.xquery(sql):
                if batch_id != _batch_id:
                    if _batch_id != 0:
                        start_batch_transfer(_batch_id, tasks, params_config, max_concurrent, db, stop_flag, threads)
        
                    _batch_id = batch_id
                    tasks = []
        
                tasks.append((tid, source, destination))
        
            if _batch_id != 0:
                start_batch_transfer(_batch_id, tasks, params_config, max_concurrent, db, stop_flag, threads)

            ## Collect terminated threads
            while True:
                ith = 0
                while ith != len(threads):
                    thread = threads[ith]
                    if thread.is_alive():
                        ith += 1
                    else:
                        thread.join()
                        threads.pop(ith)

                LOG.info('Number of batches: %d', len(threads))
                if len(threads) >= max_batches:
                    LOG.info('Waiting 10 seconds..')
                    time.sleep(10)
                else:
                    break

            time.sleep(30)

    except KeyboardInterrupt:
        stop_flag.set()
        
    except:
        log_exception(LOG)

    while True:
        ith = 0
        while ith != len(threads):
            thread = threads[ith]
            if thread.is_alive():
                ith += 1
            else:
                thread.join()
                threads.pop(ith)

        if len(threads) != 0:
            LOG.info('Number of batches: %d', len(threads))
            time.sleep(1)
        else:
            break

    LOG.info('dynamo-fileopd terminated.')
