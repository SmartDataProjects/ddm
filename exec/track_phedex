#!/usr/bin/env python

###############################################################
####### This script will spit out png files monitoring ########
####### the copy status through Phedex on three levels: #######
####### -per replica, -per request, -per site #################
#
####### yiiyama@mit.edu, bmaier@mit.edu #######################
###############################################################

import sys
import os
import time
import shutil
import rrdtool
import selinux
import collections
import csv

from glob import glob
from datetime import datetime, timedelta

from argparse import ArgumentParser

parser = ArgumentParser(description = 'Track transfers')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')

args = parser.parse_args()
sys.argv = []

from dynamo.dataformat import Configuration
import dynamo.history.impl as history_impl
import dynamo.operation.impl as operation_impl
from dynamo.utils.interface.phedex import PhEDEx
from dynamo.utils.parallel import Map
from dynamo.core.executable import make_standard_logger

config = Configuration(args.config)

LOG = make_standard_logger(config.log_level)

rrd_dir = config.rrd_path_base + '/track_phedex'
try:
    os.makedirs(rrd_dir)
except:
    pass

sites = glob(rrd_dir+'/*')
try:
    sites.remove(rrd_dir + '/total.rrd')
    sites.remove(rrd_dir + '/total_tape.rrd')
    sites.remove(rrd_dir + '/total_disk.rrd')
except:
    pass

# Interval of the rrd file timestamps
maxtime=int(time.time())-100*24*60*60

history = getattr(history_impl, config.history.module)(config.history.config)

dynamo_requests = []
sites_with_open_transfers = []
records = collections.defaultdict(set)

sites_ongoings = []
sites_total = [] # request level
sites_copied = [] # request level
sites_total_stuck = [] # dataset level
sites_copied_stuck = [] # dataset level
sites_total_ongoing = [] # dataset level
sites_copied_ongoing = [] # dataset level

# Get all ongoing dynamo transfers

for partition in config.partitions:
    partition_records = history.get_incomplete_copies(partition)    
    for record in partition_records:
        dynamo_requests.append(record.operation_id)

copy = getattr(operation_impl, config.copy_op.module)(config.copy_op.config)

interval = int(config.rrd_interval)

timestamp = int(time.time()) / interval * interval

incomplete_replicas_rrd = []

# Keeping track of total volume ...
total_volume = 0
copied_volume = 0
total_rrdfile = rrd_dir+'/total.rrd'
total_tape_rrdfile = rrd_dir+'/total_tape.rrd'
total_disk_rrdfile = rrd_dir+'/total_disk.rrd'
total_rrdfiles = [total_rrdfile,total_tape_rrdfile,total_disk_rrdfile]

start = (int(time.time()) / interval - 1) * interval

for rrdfile in total_rrdfiles:
    if not os.path.exists(rrdfile):
        rrdtool.create(rrdfile, '--start', str(start), '--step', str(interval),
                       'DS:copied:GAUGE:%d:0:U' % (interval * 800),
                       'DS:total:GAUGE:%d:0:U' % (interval * 800),
                       'RRA:LAST:0:1:%i' % 1344    )

# Now create new rrd files and create pngs
def is_transfer_stuck(rrd_file):
    stuck = 0
    
    result = rrdtool.fetch(rrd_file, "LAST")
        
    
    lasttime = result[0][1]
    firsttime = lasttime - 6*24*3600

    result = rrdtool.fetch(rrd_file, "LAST", '-s', str(firsttime), '-e', str(lasttime))

    rows = result[2]
    try:
        while rows[len(rows)-1][1] is None:
            rows = rows[:-1]

        if len(rows) > 480 and (rows[len(rows)-1][0] - rows[len(rows)-481][0])/rows[len(rows)-1][1] < 0.01:
        #480 corresponds to 5 days * 24 hours * 4 timestamps per hour
            stuck = 1
    except:
        pass

    return stuck


# Get an array of subscription ids, names, sites, copied amounts and total sizes.
# IDs are looped through the copy.copy_status object to retrieve the remaining
# properties of the subscription. Also create and write RRD files for each dataset,
# organized by destination site.

phedex = PhEDEx(config.phedex)
datasets = phedex.make_request('subscriptions', ['percent_max=99.999', 'create_since=%d' % maxtime])

ids = []

for dataset_entry in datasets:
    if 'block' in dataset_entry:
        for block_entry in dataset_entry['block']:
            for subscription in block_entry['subscription']:
                request_id = subscription["request"]
                if request_id not in dynamo_requests and request_id not in ids:
                    site = subscription["node"]
                    ids.append(request_id)
                    if site not in sites_with_open_transfers:
                        sites_with_open_transfers.append(site)
                    records[site].add(request_id)
                
    else:
        for subscription in dataset_entry['subscription']:
            request_id = subscription["request"]
            if request_id not in dynamo_requests and request_id not in ids:
                site = subscription["node"]
                ids.append(request_id)
                if site not in sites_with_open_transfers:
                    sites_with_open_transfers.append(site)
                records[site].add(request_id)

def exec_get(sitename):
    global rrd_dir
    global records
    global copy
    global incomplete_replicas_rrd
    global interval

    global sites_ongoings
    global sites_total
    global sites_copied
    global sites_total_stuck
    global sites_copied_stuck
    global sites_total_ongoing
    global sites_copied_ongoing

    # Need this here for some reason
    import rrdtool
    import csv
    
    debug_counter = 0

    ongoings = []
    site_total = 0
    site_copied = 0
    site_total_stuck = 0
    site_copied_stuck = 0
    site_total_ongoing = 0
    site_copied_ongoing = 0

    rrd_filepath = rrd_dir + '/' + str(sitename)

    if not os.path.exists(rrd_filepath):
        # Create path corresponding to site
        os.mkdir(rrd_filepath)

    try:
        os.unlink("%s/filelist.txt" % rrd_filepath)
    except:
        pass

    for record in records[sitename]:
        debug_counter += 1
        #if debug_counter > 2:
        #    break    

        request_id = record
        status = copy.copy_status(request_id)

        request_total = 0
        request_copied = 0

        for (site, dataset), (total, copied, last_update) in status.items():


            if copied is not None:# That happens sometimes for very, very, very old requests
                site_total += total
                site_copied += copied

            tmp = rrd_filepath + '/'  +  str(request_id) + '_' + dataset.replace('/', '+') + '.rrd'
            rrd_file = tmp.replace('+','',1)
            incomplete_replicas_rrd.append(rrd_file)
         
            if not os.path.exists(rrd_file) and copied != total:
                # RRD does not exist yet
                start = (int(time.time()) / interval - 1) * interval

                # Write rrd file
                try:
                    rrdtool.create(rrd_file, '--start', str(start), '--step', str(interval),
                                   'DS:copied:GAUGE:%d:0:U' % (interval * 800),
                                   'DS:total:GAUGE:%d:0:U' % (interval * 800),
                                   'RRA:LAST:0:1:%i' % 1344    )
                except:
                    pass

                # data source
                #  DS:<name>:<type>:<heartbeat>:<min>:<max>
                #  type = GAUGE: quantity that has a value at each time point
                #  heartbeat: "maximum number of seconds that may pass between two updates of this data source before the value of the data source is assumed to be *UNKNOWN*"
                #  min/max = U: unknown
                # round robin archive (RRA)
                #  RRA:<type>:<xff>:<nsteps>:<nrows>
                #  type = LAST: just use the last value, no averaging etc.
                #  xff: fraction of <nsteps> that can have UNKNOWN as the value
                #  nsteps: number of steps used for calculation
                #  nrows: number of records to keep

                # change selinux context of the RRD so that it can be read by a apache-invoked PHP script
                try:
                    selinux.chcon(rrd_file, 'unconfined_u:object_r:httpd_sys_content_t:s0')
                except:
                    pass

            try:
                # Keeping track of the request status
                request_total += total
                request_copied += copied            
                if copied != total:

                    is_stuck = is_transfer_stuck(rrd_file)
                    ongoings.append([request_id,dataset,copied,total,is_stuck])
                    site_total_stuck += is_stuck*total
                    site_copied_stuck += is_stuck*copied
                    site_total_ongoing += total
                    site_copied_ongoing += copied
                    rrdtool.update(rrd_file, '%d:%d:%d' % (timestamp, copied, total))

                else:
                    try:
                        os.unlink(rrd_file)
                    except:
                        pass
            except:
                pass
            

    sites_ongoings.append([sitename,len(ongoings)])
    sites_total.append([sitename,site_total])
    sites_copied.append([sitename,site_copied])
    sites_total_stuck.append([sitename,site_total_stuck])
    sites_copied_stuck.append([sitename,site_copied_stuck])
    sites_total_ongoing.append([sitename,site_total_ongoing])
    sites_copied_ongoing.append([sitename,site_copied_ongoing])

    with open("%s/filelist.txt" % rrd_filepath, "w") as csvfilelist:
        fieldnames = ["id","name","copied","total","stuck"]
        writer = csv.DictWriter(csvfilelist, fieldnames=fieldnames)
        writer.writerow(dict(zip(fieldnames,fieldnames)))
        for ongoing in ongoings:
            writer.writerow({"id" : ongoing[0], "name" : ongoing[1], "copied" : ongoing[2], "total" : ongoing[3], "stuck" : ongoing[4]})        

Map(config.parallel).execute(exec_get, sites_with_open_transfers)

# Creating overview files
try:
    os.unlink("%s/overview.txt" % rrd_filepath)
except:
    pass

with open("%s/overview.txt" % rrd_dir, "w") as overview:
    fieldnames = ["sitename","ongoing","total","copied","total_stuck","copied_stuck"]
    writer = csv.DictWriter(overview, fieldnames=fieldnames)
    writer.writerow(dict(zip(fieldnames,fieldnames)))
    for site in sites_with_open_transfers:
        site_total = 0
        for x in sites_total:
            if x[0] == site:
                site_total += x[1]

        if site_total == 0:
            continue

        site_ongoing = 0
        for x in sites_ongoings:
            if x[0] == site:
                site_ongoing += x[1]

        site_total_ongoing = 0
        for x in sites_total_ongoing:
            if x[0] == site:
                site_total_ongoing += x[1]

        site_copied_ongoing = 0
        for x in sites_copied_ongoing:
            if x[0] == site:
                site_copied_ongoing += x[1]

        site_total_stuck = 0
        for x in sites_total_stuck:
            if x[0] == site:
                site_total_stuck += x[1]

        site_copied_stuck = 0
        for x in sites_copied_stuck:
            if x[0] == site:
                site_copied_stuck += x[1]

        writer.writerow({"sitename" : site, "ongoing" : site_ongoing, "total" : site_total_ongoing,
                         "copied" : site_copied_ongoing, "total_stuck" : site_total_stuck, "copied_stuck" : site_copied_stuck})

total_volume = sum((x[1] for x in sites_total))
total_tape_volume = sum((x[1] for x in sites_total if "MSS" in x[0]))
total_disk_volume = sum((x[1] for x in sites_total if "MSS" not in x[0]))
copied_volume = sum((x[1] for x in sites_copied))
copied_tape_volume = sum((x[1] for x in sites_copied if "MSS" in x[0]))
copied_disk_volume = sum((x[1] for x in sites_copied if "MSS" not in x[0]))

try:
    rrdtool.update(total_rrdfile, '%d:%d:%d' % (timestamp, copied_volume, total_volume))
    rrdtool.update(total_tape_rrdfile, '%d:%d:%d' % (timestamp, copied_tape_volume, total_tape_volume))
    rrdtool.update(total_disk_rrdfile, '%d:%d:%d' % (timestamp, copied_disk_volume, total_disk_volume))
except:
    pass


# Deletion part - first delete rrd files of completed requests that are older than one week, since we do not want them to be a part of the graphs anymore 
existing_rrds = glob(rrd_dir+'/*/*.rrd')

older_than = datetime.now() - timedelta(days=20)

for existing_rrd in existing_rrds:
    filetime = datetime.fromtimestamp(os.path.getmtime(existing_rrd))
    if existing_rrd not in incomplete_replicas_rrd and filetime < older_than:
        # Delete pngs and rrd files
        os.unlink(existing_rrd)

# Copying rrds to the /var/www location
target_dir = config.rrd_publish_target + '/monitoring_phedex'
for entry in os.listdir(target_dir):
    if entry.startswith('T'):
        target = target_dir + '/' + entry
        try:
            shutil.rmtree(target)
        except:
            pass

for fname in ['total.rrd', 'total_tape.rrd', 'total_disk.rrd', 'overview.txt']:
    try:
        os.unlink(target_dir + '/' + fname)
    except:
        pass

for entry in os.listdir(rrd_dir):
    if entry.startswith('T'):
        source = rrd_dir + '/' + entry
        target = target_dir + '/' + entry
        shutil.copytree(source, target)

shutil.copy(rrd_dir + '/total.rrd', target_dir)
shutil.copy(rrd_dir + '/total_tape.rrd', target_dir)
shutil.copy(rrd_dir + '/total_disk.rrd', target_dir)
shutil.copy(rrd_dir + '/overview.txt', target_dir)


