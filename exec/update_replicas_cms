#!/usr/bin/env python

import sys
import logging
import time
import fnmatch
import re
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update replica information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')
parser.add_argument('--site', '-s', metavar = 'SITE', dest = 'site', help = 'Do a full update of the site (wildcard allowed).')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
delta_update = (not args.site and not args.dataset)

## Load the configuration
from dataformat.configuration import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout)
log_level = getattr(logging, config.log_level.upper())
log_format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s'

# Everything above log_level goes to stdout
out_handler = logging.StreamHandler(sys.stdout)
out_handler.setLevel(log_level)
out_handler.setFormatter(logging.Formatter(fmt = log_format))
# If >= ERROR, goes also to stderr
err_handler = logging.StreamHandler(sys.stderr)
err_handler.setLevel(logging.ERROR)
err_handler.setFormatter(logging.Formatter(fmt = log_format))

LOG = logging.getLogger()
LOG.setLevel(log_level)
LOG.addHandler(out_handler)
LOG.addHandler(err_handler)

LOG.info('Starting inventory update.')

## Load and initialize sources
from core.executable import inventory
import source.impl as sources

config.groups.config.phedex = config.phedex
config.sites.config.phedex = config.phedex
config.datasets.config.phedex = config.phedex
config.replicas.config.phedex = config.phedex

group_source = sources.PhEDExGroupInfoSource(config.groups.config)
site_source = sources.PhEDExSiteInfoSource(config.sites.config)
dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)
replica_source = sources.PhEDExReplicaInfoSource(config.replicas.config)

## Start the update
# 1. Refresh groups and sites
# 2. Pick up new and changed block replicas
# 3. Loop over new and changed block replicas, add them to inventory
# 4. Pick up deleted block replicas
# 5. Loop over deleted replicas
# 6. (If delta update) save the timestamp

from dataformat import DatasetReplica, ObjectError

## 1. Refresh groups and sites

LOG.info('Updating list of groups.')
for group in group_source.get_group_list():
    inventory.update(group)

LOG.info('Updating list of sites.')
for site in site_source.get_site_list():
    inventory.update(site)

## 2. Pick up new and changed block replicas

if delta_update:
    ## Normal global delta-update
    ## Get the last update timestamp

    try:
        with open(config.replica_state_file) as source:
            timestamp = time.strptime(source.read().strip(), '%Y-%m-%d %H:%M:%S %Z')
            # %Z only serves to distinguish daylight saving time. The timestamp in the file has
            # to be the local time of the system. time.mktime will then convert it to UNIX time.
            last_update = time.mktime(timestamp)
    except:
        LOG.error('Failed to retrieve last update timestamp.')
        sys.exit(1)

    update_start = time.time()
    
    ## Fetch the full list of block replicas that were updated since updated_since.
    ## New datasets and blocks will be caught in the process.
    
    LOG.info('Updating list of datasets, blocks, and replicas.')

    new_and_updated_replicas = replica_source.get_updated_replicas(last_update)

else:
    new_and_updated_replicas = replica_source.get_replicas(site = args.site, dataset = args.dataset)
    # Save the embedded versions
    embedded_updated_replicas = set()

## 3. Loop over new and changed block replicas, add them to inventory

for replica in new_and_updated_replicas:
    replica_str = str(replica)

    LOG.debug('Updating %s', replica_str)

    # 3.1. pick up replicas of known groups only
    if replica.group.name not in inventory.groups:
        LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
        continue

    # 3.2. Pick up replicas at known sites only
    try:
        site = inventory.sites[replica.site.name]
    except KeyError:
        LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
        continue

    # 3.3. Find the dataset in the inventory

    try:
        dataset = inventory.datasets[replica.block.dataset.name]
    except KeyError:
        # If not found, create a new dataset and inject
        LOG.info('Unknown dataset %s. Collecting information.', replica.block.dataset.name)

        dataset_tmp = dataset_source.get_dataset(replica.block.dataset.name)
        if dataset_tmp is None:
            LOG.error('Unknown dataset %s.', replica.block.dataset.name)
            continue

        inventory.update(dataset_tmp)

        # This is the dataset that is embedded in the inventory
        dataset = inventory.datasets[replica.block.dataset.name]

        # Inject blocks as well
        for block in dataset_tmp.blocks:
            inventory.update(block)
    else:
        # If found, update the information (size, last_update, etc.) if necessary
        LOG.debug('Updating information for dataset %s', dataset.name)
        inventory.update(replica.block.dataset)

    # 3.4. Find the block of the dataset

    block_full_name = replica.block.full_name()
    block = dataset.find_block(replica.block.name)

    if block is None:
        # If not found, create a new block and inject
        LOG.debug('Unknown block %s. Collecting information.', block_full_name)

        block_tmp = dataset_source.get_block(block_full_name, dataset)
        if block_tmp is None:
            LOG.error('Unknown block %s.', block_full_name)
            continue

        inventory.update(block_tmp)

        # This is the block that is embedded in the inventory (`block.dataset` points to `dataset`)
        block = dataset.find_block(replica.block.name)

        dataset.size += block.size
        dataset.num_files += block.num_files

        # Update dataset information
        inventory.update(dataset)
    else:
        # If found, update the information if necessary
        LOG.debug('Updating information for block %s', block_full_name)
        inventory.update(replica.block)

    # 3.5. Find the dataset replica

    if site.find_dataset_replica(dataset) is None:
        # If not found, create a new replica and inject
        LOG.debug('Creating new replica of %s at %s', dataset.name, site.name)
        inventory.update(DatasetReplica(dataset, site))

    # 3.6. Update the block replica

    LOG.debug('Updating block replica.')
    embedded_replica = inventory.update(replica)

    if not delta_update:
        embedded_updated_replicas.add(embedded_replica)

## 4. Pick up deleted block replicas

if delta_update:
    # last_update should be available
    deleted_replicas = replica_source.get_deleted_replicas(last_update)

else:
    # Replicas in inventory but not in new_and_updated_replicas are deleted
    deleted_replicas = []

    if args.site:
        site_pat = re.compile(fnmatch.translate(args.site))
    else:
        site_pat = None

    if args.dataset:
        dataset_pat = re.compile(fnmatch.translate(args.dataset))
    else:
        dataset_pat = None

    for site in inventory.sites.itervalues():
        if site_pat and not site_pat.match(site.name):
            continue

        for dataset_replica in site.dataset_replicas():
            if dataset_pat and not dataset_pat.match(dataset_replica.dataset.name):
                continue

            for block_replica in dataset_replica.block_replicas:
                if block_replica not in embedded_updated_replicas:
                    deleted_replicas.append(block_replica)

## 5. Loop over deleted replicas

for replica in deleted_replicas:
    replica_str = str(replica)

    LOG.debug('Deleting %s', replica_str)

    # 5.1. pick up replicas of known groups only
    if replica.group.name not in inventory.groups:
        LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
        continue

    # 5.2. Pick up replicas at known sites only
    try:
        site = inventory.sites[replica.site.name]
    except KeyError:
        LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
        continue

    # 5.3. Find the dataset in the inventory

    try:
        dataset = inventory.datasets[replica.block.dataset.name]
    except KeyError:
        # If not found, create a new dataset and inject
        LOG.debug('Unknown dataset %s.', replica.block.dataset.name)
        continue

    # 5.4. Find the block of the dataset

    block_full_name = replica.block.full_name()
    block = dataset.find_block(replica.block.name)

    if block is None:
        # If not found, create a new block and inject
        LOG.debug('Unknown block %s.', block_full_name)
        continue

    # 5.5. Find the dataset replica

    dataset_replica = site.find_dataset_replica(dataset)
    if dataset_replica is None:
        LOG.debug('No replica of %s at %s.', dataset.name, site.name)
        continue

    # 5.6. Delete the block replica

    # blockreplica.delete_from() raises a KeyError or ObjectError if
    # any of the group, site, dataset, ... is not found
    try:
        inventory.delete(replica)
    except (KeyError, ObjectError):
        LOG.debug('Replica not found.')
        pass

    # 5.7. Delete the dataset replica if it is empty

    if len(dataset_replica.block_replicas) == 0:
        inventory.delete(dataset_replica)

## 6. (If delta update) save the timestamp

if delta_update:
    ## Store the timestamp of when we started (allow 1 minute safety margin)
    with open(config.replica_state_file, 'w') as source:
        local_time = time.localtime(update_start - 60)
        source.write(time.strftime('%Y-%m-%d %H:%M:%S %Z\n', local_time))

LOG.info('Inventory update completed.')
