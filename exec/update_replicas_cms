#!/usr/bin/env python

import sys
import logging
import time
import fnmatch
import re
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update replica information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', required = True, help = 'Configuration JSON.')
parser.add_argument('--site', '-s', metavar = 'SITE', dest = 'site', nargs = '?', const = '', help = 'Do a full update of the site (wildcard allowed). If no site is specified, pick up the site name from the state file.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
delta_update = (args.site is None and args.dataset is None)
rolling_site_update = False

## Load the configuration
from dynamo.dataformat.configuration import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)
from dynamo.core.executable import make_standard_logger

LOG = make_standard_logger(config.log_level)
LOG.info('Starting inventory update.')

## Load and initialize sources
from dynamo.core.executable import inventory
import dynamo.source.impl as sources

config.groups.config.phedex = config.phedex
config.sites.config.phedex = config.phedex
config.datasets.config.phedex = config.phedex
config.replicas.config.phedex = config.phedex

group_source = sources.PhEDExGroupInfoSource(config.groups.config)
site_source = sources.PhEDExSiteInfoSource(config.sites.config)
dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)
replica_source = sources.PhEDExReplicaInfoSource(config.replicas.config)

## Start the update
# 1. Refresh groups and sites
# 2. Pick up new and changed block replicas
# 3. Loop over new and changed block replicas, add them to inventory
# 4. Pick up deleted block replicas
# 5. Loop over deleted replicas
# 6. (If delta update) save the timestamp

from dynamo.dataformat import DatasetReplica, ObjectError

## 1. Refresh groups and sites

LOG.info('Updating list of groups.')
for group in group_source.get_group_list():
    LOG.debug('Updating %s', str(group))
    inventory.update(group)

LOG.info('Updating list of sites.')
for site in site_source.get_site_list():
    LOG.debug('Updating %s', str(site))
    inventory.update(site)

## 2. Pick up new and changed block replicas

if delta_update:
    ## Normal global delta-update
    ## Get the last update timestamp

    try:
        with open(config.replica_state_file) as source:
            # first line is the timestamp
            timestamp_str = source.readline().strip()
            timestamp = time.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S %Z')
            # %Z only serves to distinguish daylight saving time. The timestamp in the file has
            # to be the local time of the system. time.mktime will then convert it to UNIX time.
            last_update = time.mktime(timestamp)
    except:
        LOG.error('Failed to retrieve last update timestamp.')
        sys.exit(1)

    update_start = time.time()
    
    ## Fetch the full list of block replicas that were updated since updated_since.
    ## New datasets and blocks will be caught in the process.
    
    LOG.info('Fetching the list of block replicas updated since %s.', timestamp_str)

    new_and_updated_replicas = replica_source.get_updated_replicas(last_update)

else:
    if args.site == '':
        rolling_site_update = True

        try:
            with open(config.replica_state_file) as source:
                # second line is the site name
                source.readline()
                args.site = source.readline().strip()
        except:
            LOG.error('Failed to retrieve site name.')
            sys.exit(1)

    new_and_updated_replicas = replica_source.get_replicas(site = args.site, dataset = args.dataset)
    # Save the embedded versions
    embedded_updated_replicas = set()

## 3. Loop over new and changed block replicas, add them to inventory

num_replicas = len(new_and_updated_replicas)
LOG.info('Got %d block replicas to update.', num_replicas)

updated_datasets = set()
updated_blocks = set()

watermark = 0

for irep, replica in enumerate(new_and_updated_replicas):
    if float(irep) / num_replicas * 100. >= watermark:
        LOG.info('%d%% done..', watermark)
        watermark += 5

    replica_str = str(replica)

    LOG.debug('Updating %s', replica_str)

    # 3.1. pick up replicas of known groups only

    if replica.group.name not in inventory.groups:
        LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
        continue

    # 3.2. Pick up replicas at known sites only

    try:
        site = inventory.sites[replica.site.name]
    except KeyError:
        LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
        continue

    # 3.3. Update the dataset info

    dataset_name = replica.block.dataset.name

    if dataset_name not in updated_datasets:
        LOG.info('Updating information for dataset %s', dataset_name)

        dataset_tmp = dataset_source.get_dataset(dataset_name)
        if dataset_tmp is None:
            LOG.error('Unknown dataset %s.', dataset_name)
            continue

        dataset = inventory.update(dataset_tmp)

        # Inject blocks as well
        for block_tmp in dataset_tmp.blocks:
            inventory.update(block_tmp)

        updated_datasets.add(dataset_name)

    else:
        dataset = inventory.datasets[dataset_name]

    # 3.4. Find the block of the dataset

    block = dataset.find_block(replica.block.name)

    if block is None:
        LOG.error('Unknown block %s.', replica.block.full_name())
        continue

    # 3.5. Find the dataset replica

    if site.find_dataset_replica(dataset) is None:
        # If not found, create a new replica and inject
        LOG.info('Creating new replica of %s at %s', dataset.name, site.name)
        inventory.update(DatasetReplica(dataset, site))

    # 3.6. Update the block replica

    LOG.debug('Updating block replica.')
    embedded_replica = inventory.update(replica)

    if not delta_update:
        embedded_updated_replicas.add(embedded_replica)

## 4. Pick up deleted block replicas

if delta_update:
    # last_update should be available
    deleted_replicas = replica_source.get_deleted_replicas(last_update)

else:
    # Replicas in inventory but not in new_and_updated_replicas are deleted
    deleted_replicas = []

    if args.site:
        site_pat = re.compile(fnmatch.translate(args.site))
    else:
        site_pat = None

    if args.dataset:
        dataset_pat = re.compile(fnmatch.translate(args.dataset))
    else:
        dataset_pat = None

    for site in inventory.sites.itervalues():
        if site_pat and not site_pat.match(site.name):
            continue

        for dataset_replica in site.dataset_replicas():
            if dataset_pat and not dataset_pat.match(dataset_replica.dataset.name):
                continue

            for block_replica in dataset_replica.block_replicas:
                if block_replica not in embedded_updated_replicas:
                    deleted_replicas.append(block_replica)

## 5. Loop over deleted replicas

for replica in deleted_replicas:
    replica_str = str(replica)

    LOG.debug('Deleting %s', replica_str)

    # 5.1. pick up replicas of known groups only

    if replica.group.name not in inventory.groups:
        LOG.debug('%s is owned by %s, which is not a tracked group.', replica_str, replica.group.name)
        continue

    # 5.2. Pick up replicas at known sites only

    try:
        site = inventory.sites[replica.site.name]
    except KeyError:
        LOG.debug('%s is at %s, which is not a tracked site.', replica_str, replica.site.name)
        continue

    # 5.3. Find the dataset in the inventory

    try:
        dataset = inventory.datasets[replica.block.dataset.name]
    except KeyError:
        # If not found, create a new dataset and inject
        LOG.debug('Unknown dataset %s.', replica.block.dataset.name)
        continue

    # 5.4. Find the block of the dataset

    block_full_name = replica.block.full_name()
    block = dataset.find_block(replica.block.name)

    if block is None:
        # If not found, create a new block and inject
        LOG.debug('Unknown block %s.', block_full_name)
        continue

    # 5.5. Find the dataset replica

    dataset_replica = site.find_dataset_replica(dataset)
    if dataset_replica is None:
        LOG.debug('No replica of %s at %s.', dataset.name, site.name)
        continue

    # 5.6. Delete the block replica

    # blockreplica.delete_from() raises a KeyError or ObjectError if
    # any of the group, site, dataset, ... is not found
    try:
        inventory.delete(replica)
    except (KeyError, ObjectError):
        LOG.debug('Replica not found.')
        pass

    # 5.7. Delete the dataset replica if it is empty

    if len(dataset_replica.block_replicas) == 0:
        LOG.info('Deleting replica %s:%s', site.name, dataset.name)
        inventory.delete(dataset_replica)

## 6. (If delta update) save the timestamp

if delta_update:
    ## Store the timestamp of when we started (allow 1 minute safety margin)
    with open(config.replica_state_file) as source:
        lines = source.read().split('\n')

    local_time = time.localtime(update_start - 60)
    lines[0] = time.strftime('%Y-%m-%d %H:%M:%S %Z', local_time)

    with open(config.replica_state_file, 'w') as source:
        source.write('\n'.join(lines))

elif rolling_site_update:
    with open(config.replica_state_file) as source:
        lines = source.read().split('\n')

    sites = sorted(inventory.sites.keys())
    isite = (sites.index(args.site) + 1) % len(sites)
    lines[1] = sites[isite]

    with open(config.replica_state_file, 'w') as source:
        source.write('\n'.join(lines))

LOG.info('Inventory update completed.')
