#!/usr/bin/env python

import sys
import logging
import time
import tempfile
import shutil
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update datasets, blocks, and files information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', default = '', help = 'Configuration JSON.')
parser.add_argument('--log-level', '-l', metavar = 'LEVEL', dest = 'log_level', default = 'INFO', help = 'Logging level.')
parser.add_argument('--num-datasets', '-n', metavar = 'N', dest = 'num_datasets', type = int, default = 10, help = 'Number of datasets to update in one cycle, if doing a rolling update.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
rolling_update = (not args.dataset)

## Set up logging (write to stderr)

log_level = getattr(logging, args.log_level.upper())
log_format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s'

logging.basicConfig(level = log_level, format = log_format)
LOG = logging.getLogger()

## Load and initialize sources

from core.executable import inventory
from dataformat import Configuration
import source.impl as sources

config = Configuration(args.config)

config.datasets.config.phedex = config.phedex

dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)

## Get the list of every dataset ever existed
def generate_dataset_list(dataset_source, state_file):
    all_datasets = dataset_source.get_dataset_names(include = ['/*/*/*'])
    with open(state_file, 'w') as dataset_list:
        for dataset in all_datasets:
            dataset_list.write(dataset + '\n')

## Pick up datasets to inquire

if rolling_update:
    LOG.info('Updating status of %d datasets', args.num_datasets)

    datasets_to_update = []
    list_exhausted = False

    if not os.path.exists(config.dataset_state_file):
        LOG.info('List of dataset names not found. Generating a new list.')
        generate_dataset_list(dataset_source, config.dataset_state_file)

    with open(config.dataset_state_file) as source:
        for line in source:
            datasets_to_update.append(line.strip())
            
            if len(datasets_to_update) == args.num_datasets:
                break
        else:
            list_exhausted = True

else:
    datasets_to_update = dataset_source.get_dataset_names(include = [args.dataset])

for dataset_name in datasets_to_update:
    try:
        inventory_dataset = inventory.datasets[dataset_name]
    except KeyError:
        inventory_dataset = Dataset(dataset_name)

    LOG.debug('Updating %s', dataset_name)

    source_dataset = dataset_source.get_dataset(dataset_name)

    # if some dataset attribute changed
    inventory.update(source_dataset)

    inventory_block_names = set(b.name for b in inventory_dataset.blocks)
    source_block_names = set(b.name for b in source_dataset.blocks)

    for block_name in source_block_names:
        source_block = source_dataset.find_block(block_name)
        inventory.update(source_block)

    for block_name in (inventory_block_names - source_block_names):
        inventory_block = inventory_dataset.find_block(block_name)
        inventory.delete(inventory_block)

if rolling_update:
    if list_exhausted:
        generate_dataset_list(dataset_source, config.dataset_state_file)
    else:
        with open(config.dataset_state_file) as source:
            with tempfile.NamedTemporaryFile(mode = 'w') as dataset_list:
                for line in source:
                    if line in datasets_to_update:
                        continue

                    dataset_list.write(line)

                tmp_name = dataset_list.name
        
        os.unlink(config.dataset_state_file)
        shutil.move(tmp_name, config.dataset_state_file)

LOG.info('Dataset update completed.')
