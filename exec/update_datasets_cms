#!/usr/bin/env python

import sys
import logging
import time
import tempfile
import shutil
import sqlite3
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update datasets, blocks, and files information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', default = '', help = 'Configuration JSON.')
parser.add_argument('--log-level', '-l', metavar = 'LEVEL', dest = 'log_level', default = 'INFO', help = 'Logging level.')
parser.add_argument('--num-datasets', '-n', metavar = 'N', dest = 'num_datasets', type = int, default = 10, help = 'Number of datasets to update in one cycle, if doing a rolling update.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
rolling_update = (not args.dataset)

## Load the configuration
from dynamo.dataformat import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)
from dynamo.core.executable import make_standard_logger

LOG = make_standard_logger(config.log_level)

## Load and initialize sources
from dynamo.core.executable import inventory
import dynamo.source.impl as sources

config.datasets.config.phedex = config.phedex

dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)

## Start the update
# 1. Pick up datasets to inquire
# 2. Update dataset and block information
# 3. (If rolling update) remove updated datasets from the list

from dynamo.dataformat import Dataset

## 1. Pick up datasets to inquire

if rolling_update:
    LOG.info('Updating status of %d datasets', args.num_datasets)

    datasets_to_update = []
    list_exhausted = False

    if not os.path.exists(config.dataset_state_file):
        LOG.info('List of dataset names not found. Run generate_dataset_list_cms.')
        sys.exit(0)

    state_db = sqlite3.connect(config.dataset_state_file)
    cursor = state_db.cursor()
    for row in cursor.execute('SELECT `name` FROM `datasets` ORDER BY `id` LIMIT ?', (args.num_datasets,)):
        datasets_to_update.append(str(row[0]))

    cursor.close()
    state_db.close()

else:
    datasets_to_update = dataset_source.get_dataset_names(include = [args.dataset])

## 2. Update dataset and block information

for dataset_name in datasets_to_update:
    try:
        inventory_dataset = inventory.datasets[dataset_name]
    except KeyError:
        inventory_dataset = Dataset(dataset_name)

    if rolling_update:
        LOG.info('Updating %s', dataset_name)
    else:
        LOG.debug('Updating %s', dataset_name)

    source_dataset = dataset_source.get_dataset(dataset_name)
    if source_dataset is None:
        LOG.error('%s not found in dataset source.', dataset_name)
        continue

    # if some dataset attribute changed
    inventory.update(source_dataset)

    inventory_block_names = set(b.name for b in inventory_dataset.blocks)
    source_block_names = set()

    for block in source_dataset.blocks:
        inventory.update(block)
        source_block_names.add(block.name)

    for block_name in (inventory_block_names - source_block_names):
        inventory_block = inventory_dataset.find_block(block_name)
        inventory.delete(inventory_block)

## 3. (If rolling update) remove updated datasets from the list

if rolling_update:
    LOG.info('Updating the dataset list.')

    state_db = sqlite3.connect(config.dataset_state_file)
    cursor = state_db.cursor()
    cursor.execute('DELETE FROM `datasets` WHERE `id` IN (SELECT `id` FROM `datasets` ORDER BY `id` LIMIT ?)', (args.num_datasets,))
    result = cursor.execute('SELECT COUNT(*) FROM `datasets`')
    count = next(result)[0]
    cursor.close()
    state_db.close()

    if count == 0:
        LOG.info('Reached the end of dataset list. Deleting the state DB file.')
        os.unlink(config.dataset_state_file)

LOG.info('Dataset update completed.')
