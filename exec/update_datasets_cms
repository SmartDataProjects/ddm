#!/usr/bin/env python

import sys
import logging
import time
import tempfile
import shutil
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update datasets, blocks, and files information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', default = '', help = 'Configuration JSON.')
parser.add_argument('--log-level', '-l', metavar = 'LEVEL', dest = 'log_level', default = 'INFO', help = 'Logging level.')
parser.add_argument('--num-datasets', '-n', metavar = 'N', dest = 'num_datasets', type = int, default = 10, help = 'Number of datasets to update in one cycle, if doing a rolling update.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
rolling_update = (not args.dataset)

## Load the configuration
from dataformat import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout)
log_level = getattr(logging, config.log_level.upper())
log_format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s'

# Everything above log_level goes to stdout
out_handler = logging.StreamHandler(sys.stdout)
out_handler.setLevel(log_level)
out_handler.setFormatter(logging.Formatter(fmt = log_format))
# If >= ERROR, goes also to stderr
err_handler = logging.StreamHandler(sys.stderr)
err_handler.setLevel(logging.ERROR)
err_handler.setFormatter(logging.Formatter(fmt = log_format))

LOG = logging.getLogger()
LOG.setLevel(log_level)
LOG.addHandler(out_handler)
LOG.addHandler(err_handler)

## Load and initialize sources
from core.executable import inventory
import source.impl as sources

config.datasets.config.phedex = config.phedex

dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)

## Start the update
# 1. Pick up datasets to inquire
# 2. Update dataset and block information
# 3. (If rolling update) remove updated datasets from the list

from dataformat import Dataset

## Routine to get the list of every dataset that ever existed
def generate_dataset_list(dataset_source, state_file):
    all_datasets = dataset_source.get_dataset_names(include = ['/*/*/*'])
    with open(state_file, 'w') as dataset_list:
        for dataset in all_datasets:
            dataset_list.write(dataset + '\n')

## 1. Pick up datasets to inquire

if rolling_update:
    LOG.info('Updating status of %d datasets', args.num_datasets)

    datasets_to_update = []
    list_exhausted = False

    if not os.path.exists(config.dataset_state_file):
        LOG.info('List of dataset names not found. Generating a new list.')
        generate_dataset_list(dataset_source, config.dataset_state_file)

    with open(config.dataset_state_file) as source:
        for line in source:
            datasets_to_update.append(line.strip())
            
            if len(datasets_to_update) == args.num_datasets:
                break
        else:
            list_exhausted = True

else:
    datasets_to_update = dataset_source.get_dataset_names(include = [args.dataset])

## 2. Update dataset and block information

for dataset_name in datasets_to_update:
    try:
        inventory_dataset = inventory.datasets[dataset_name]
    except KeyError:
        inventory_dataset = Dataset(dataset_name)

    LOG.debug('Updating %s', dataset_name)

    source_dataset = dataset_source.get_dataset(dataset_name)
    if source_dataset is None:
        LOG.error('%s not found in dataset source.', dataset_name)
        continue

    # if some dataset attribute changed
    inventory.update(source_dataset)

    inventory_block_names = set(b.name for b in inventory_dataset.blocks)
    source_block_names = set()

    for block in source_dataset.blocks:
        inventory.update(block)
        source_block_names.add(block.name)

    for block_name in (inventory_block_names - source_block_names):
        inventory_block = inventory_dataset.find_block(block_name)
        inventory.delete(inventory_block)

## 3. (If rolling update) remove updated datasets from the list

if rolling_update:
    if list_exhausted:
        generate_dataset_list(dataset_source, config.dataset_state_file)
    else:
        with open(config.dataset_state_file) as source:
            with tempfile.NamedTemporaryFile(mode = 'w') as dataset_list:
                for line in source:
                    if line in datasets_to_update:
                        continue

                    dataset_list.write(line)

                tmp_name = dataset_list.name
        
        os.unlink(config.dataset_state_file)
        shutil.move(tmp_name, config.dataset_state_file)

LOG.info('Dataset update completed.')
