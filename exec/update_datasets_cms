#!/usr/bin/env python

import os
import sys
import sqlite3
from argparse import ArgumentParser

parser = ArgumentParser(description = 'Update datasets, blocks, and files information.')
parser.add_argument('--config', '-c', metavar = 'CONFIG', dest = 'config', default = '', help = 'Configuration JSON.')
parser.add_argument('--dataset', '-d', metavar = 'DATASET', dest = 'dataset', help = 'Do a full update of the dataset (wildcard allowed).')

args = parser.parse_args()
sys.argv = []

## Type of update
rolling_update = (not args.dataset)

## Load the configuration
from dynamo.dataformat import Configuration

config = Configuration(args.config)

## Set up logging (write to stdout & stderr)
import dynamo.core.executable as executable

LOG = executable.make_standard_logger(config.log_level)

## Load and initialize sources
import dynamo.source.impl as sources

inventory = executable.inventory
config.datasets.config.phedex = config.phedex

dataset_source = sources.PhEDExDatasetInfoSource(config.datasets.config)

## Start the update
# 1. Pick up datasets to inquire
# 2. Update dataset and block information
# 3. (If rolling update) remove updated datasets from the list

from dynamo.dataformat import Dataset

## 1. Pick up datasets to inquire

if rolling_update:
    LOG.info('Updating status of %d datasets', config.num_update_datasets)

    datasets_to_update = []
    list_exhausted = False

    if not os.path.exists(config.dataset_state_file):
        LOG.info('List of dataset names not found. Run generate_dataset_list_cms.')
        sys.exit(1)

    state_db = sqlite3.connect(config.dataset_state_file)
    cursor = state_db.cursor()
    for row in cursor.execute('SELECT `name` FROM `datasets` ORDER BY `id` LIMIT ?', (config.num_update_datasets,)):
        datasets_to_update.append(str(row[0]))

    state_db.close()

else:
    datasets_to_update = dataset_source.get_dataset_names(include = [args.dataset])

## 2. Update dataset and block information

for dataset_name in datasets_to_update:
    LOG.info('Updating information for dataset %s', dataset_name)
    
    try:
        dataset = inventory.datasets[dataset_name]
        existing_block_names = set(b.name for b in dataset.blocks)
    except KeyError:
        existing_block_names = set()

    dataset_tmp = dataset_source.get_dataset(dataset_name)
    if dataset_tmp is None:
        LOG.error('Unknown dataset %s.', dataset_name)
        continue

    dataset = inventory.update(dataset_tmp)

    # Inject blocks as well
    updated_block_names = set()
    for block_tmp in dataset_tmp.blocks:
        block = inventory.update(block_tmp)
        updated_block_names.add(block_tmp.name)

        existing_file_names = set(f.lfn for f in block.files)

        updated_file_names = set()
        for file_tmp in dataset_source.get_files(block_tmp):
            inventory.update(file_tmp)
            updated_file_names.add(file_tmp.lfn)

        # Delete excess files
        for file_name in (existing_file_names - updated_file_names):
            lfile = block.find_file(file_name)
            inventory.delete(lfile)

    # Delete excess blocks if there are any
    for block_name in (existing_block_names - updated_block_names):
        block = dataset.find_block(block_name)
        inventory.delete(block)

## 3. (If rolling update) remove updated datasets from the list

if not executable.read_only and rolling_update:
    LOG.info('Updating the dataset list.')

    state_db = sqlite3.connect(config.dataset_state_file)
    cursor = state_db.cursor()
    cursor.execute('DELETE FROM `datasets` WHERE `id` IN (SELECT `id` FROM `datasets` ORDER BY `id` LIMIT ?)', (config.num_update_datasets,))
    state_db.commit()

    result = cursor.execute('SELECT COUNT(*) FROM `datasets`')
    count = next(result)[0]

    state_db.close()

    if count == 0:
        LOG.info('Reached the end of dataset list. Run generate_dataset_list_cms to refill.')

LOG.info('Dataset update completed.')
