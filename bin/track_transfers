#!/usr/bin/env python

import sys
import os
import time
import rrdtool
import selinux
import subprocess
import random
import collections
import csv
import commands

from glob import glob
from datetime import datetime, timedelta

import common.configuration as config
import common.interface.classes as classes
from common.misc import parallel_exec


###############################################################
####### This script will spit out png files monitoring ########
####### the copy status through Phedex on three levels: #######
####### -per replica, -per request, -per site #################
#
####### yiiyama@mit.edu, bmaier@mit.edu #######################
###############################################################

if int(commands.getstatusoutput('ps -Af | grep "python /usr/local/dynamo/bin/track_transfers" | wc -l')[1]) > 3:
    print "Found another running instance of track_transfers. Exiting."
    sys.exit(1)

DEBUG = 0

partitions = ['AnalysisOps','Physics']

rrd_dir = config.paths.data + '/track_transfers'
sites = glob(rrd_dir+'/*')
sites.remove(config.paths.data + '/track_transfers/monitoring')
try:
    sites.remove(config.paths.data + '/track_transfers/total.rrd')
except:
    pass

# Interval of the rrd file timestamps
interval = 900

try:
    os.makedirs(rrd_dir)
except:
    pass

history = classes.default_interface['history']()

sites_with_open_transfers = []
records = collections.defaultdict(set)

sites_ongoings = []
sites_total = []
sites_copied = []
sites_total_stuck = []
sites_copied_stuck = []

# Get all sites with ongoing transfers

for partition in partitions:
    partition_records = history.get_incomplete_copies(partition)    
    for record in partition_records:
        site = history.get_site_name(record.operation_id)
        records[site].add(record)
        if site not in sites_with_open_transfers:
            sites_with_open_transfers.append(site)


if DEBUG:
    print 'Incomplete copies at the moment: ', len(request_ids) 

copy = classes.default_interface['copy']()

timestamp = int(time.time()) / interval * interval

incomplete_replicas_rrd = []

# Keeping track of total volume ...
total_volume = 0
copied_volume = 0
total_rrdfile = rrd_dir+'/total.rrd'


start = (int(time.time()) / interval - 1) * interval

if not os.path.exists(total_rrdfile):
    
    rrdtool.create(total_rrdfile, '--start', str(start), '--step', str(interval),
                   'DS:copied:GAUGE:%d:0:U' % (interval * 800),
                   'DS:total:GAUGE:%d:0:U' % (interval * 800),
                   'RRA:LAST:0:1:%i' % 1344    )

# Now create new rrd files and create pngs
def is_transfer_stuck(rrd_file):
    stuck = 0
    result = rrdtool.fetch(rrd_file, "LAST")
    rows = result[2]
    while rows[len(rows)-1][1] is None:
        rows = rows[:-1]

    if len(rows) > 288 and (rows[len(rows)-1][0] - rows[len(rows)-289][0])/rows[len(rows)-1][1] < 0.01:
        #288 corresponds to 3 days * 24 hours * 4 timestamps per hour
        stuck = 1

    return stuck

def exec_get(sitename):
    debug_counter = 0

    ongoings = []
    site_total = 0
    site_copied = 0
    site_total_stuck = 0
    site_copied_stuck = 0

    rrd_filepath = rrd_dir + '/' + sitename

    if not os.path.exists(rrd_filepath):
        # Create path corresponding to site
        subprocess.call("mkdir %s" % rrd_filepath, shell = True)

    try:
        subprocess.call("rm -f %s/filelist.txt" % rrd_filepath, shell = True)
    except:
        pass

    for record in records[sitename]:
        debug_counter += 1
        #if debug_counter > 2:
        #    break    

        request_id = record.operation_id
        status = copy.copy_status(request_id)

        request_total = 0
        request_copied = 0

        for (site, dataset), (total, copied, last_update) in status.items():

            if copied is not None:# That happens sometimes for very, very, very old requests
                #sites_total.append([site,total])
                #sites_copied.append([site,copied])
                site_total += total
                site_copied += copied

            tmp = rrd_filepath + '/'  +  str(request_id) + '_' + dataset.replace('/', '+') + '.rrd'
            rrd_file = tmp.replace('+','',1)
            incomplete_replicas_rrd.append(rrd_file)
         
            if not os.path.exists(rrd_file) and copied != total:
                # RRD does not exist yet
                start = (int(time.time()) / interval - 1) * interval

                # Write rrd file
                rrdtool.create(rrd_file, '--start', str(start), '--step', str(interval),
                               'DS:copied:GAUGE:%d:0:U' % (interval * 800),
                               'DS:total:GAUGE:%d:0:U' % (interval * 800),
                               'RRA:LAST:0:1:%i' % 1344    )

                # data source
                #  DS:<name>:<type>:<heartbeat>:<min>:<max>
                #  type = GAUGE: quantity that has a value at each time point
                #  heartbeat: "maximum number of seconds that may pass between two updates of this data source before the value of the data source is assumed to be *UNKNOWN*"
                #  min/max = U: unknown
                # round robin archive (RRA)
                #  RRA:<type>:<xff>:<nsteps>:<nrows>
                #  type = LAST: just use the last value, no averaging etc.
                #  xff: fraction of <nsteps> that can have UNKNOWN as the value
                #  nsteps: number of steps used for calculation
                #  nrows: number of records to keep

                # change selinux context of the RRD so that it can be read by a apache-invoked PHP script
                try:
                    selinux.chcon(rrd_file, 'unconfined_u:object_r:httpd_sys_content_t:s0')
                except:
                    pass

            try:
                # Keeping track of the request status
                request_total += total
                request_copied += copied            
                if copied != total:
                    is_stuck = is_transfer_stuck(rrd_file)
                    ongoings.append([request_id,dataset,copied,total,is_stuck])
                    site_total_stuck += is_stuck*total
                    site_copied_stuck += is_stuck*copied
                    rrdtool.update(rrd_file, '%d:%d:%d' % (timestamp, copied, total))
                else:
                    try:
                        subprocess.call("rm -f %s" % rrd_file, shell = True)
                    except:
                        pass
            except:
                pass
            
        # Update history DB
        if len(status) != 0:
            if request_total != record.size or request_copied == request_total:
                record.size = request_total
                record.completed = (request_copied == request_total)        
                history.update_copy_entry(record)


    sites_ongoings.append([sitename,len(ongoings)])
    sites_total.append([sitename,site_total])
    sites_copied.append([sitename,site_copied])
    sites_total_stuck.append([sitename,site_total_stuck])
    sites_copied_stuck.append([sitename,site_copied_stuck])

    with open("%s/filelist.txt" % rrd_filepath, "w") as csvfilelist:
        fieldnames = ["id","name","copied","total","stuck"]
        writer = csv.DictWriter(csvfilelist, fieldnames=fieldnames)
        writer.writerow(dict(zip(fieldnames,fieldnames)))
        for ongoing in ongoings:
            writer.writerow({"id" : ongoing[0], "name" : ongoing[1], "copied" : ongoing[2], "total" : ongoing[3], "stuck" : ongoing[4]})        


parallel_exec(exec_get, sites_with_open_transfers, num_threads = min(64, len(sites_with_open_transfers)), print_progress = True, timeout = 3600)

# Creating overview files
try:
    subprocess.call("rm -f %s/overview.txt" % rrd_filepath, shell = True)
except:
    pass

with open("%s/overview.txt" % rrd_dir, "w") as overview:
    fieldnames = ["sitename","ongoing","total","copied","total_stuck","copied_stuck"]
    writer = csv.DictWriter(overview, fieldnames=fieldnames)
    writer.writerow(dict(zip(fieldnames,fieldnames)))
    for site in sites_with_open_transfers:
        if sum(x[1] for x in sites_total if x[0] == site) == 0:
            continue
        writer.writerow({"sitename" : site, "ongoing" : sum(x[1] for x in sites_ongoings if x[0] == site), "total" : sum(x[1] for x in sites_total if x[0] == site), 
                         "copied" : sum(x[1] for x in sites_copied if x[0] == site), "total_stuck" : sum(x[1] for x in sites_total_stuck if x[0] == site), "copied_stuck" : sum(x[1] for x in sites_copied_stuck if x[0] == site)})

total_volume = sum((x[1] for x in sites_total))
copied_volume = sum((x[1] for x in sites_copied))

try:
    rrdtool.update(total_rrdfile, '%d:%d:%d' % (timestamp, copied_volume, total_volume))
except:
    pass


# Deletion part - first delete rrd files of completed requests that are older than one week, since we do not want them to be a part of the graphs anymore 
existing_rrds = glob(rrd_dir+'/*/*.rrd')

older_than = datetime.now() - timedelta(days=20)

for existing_rrd in existing_rrds:
    filetime = datetime.fromtimestamp(os.path.getmtime(existing_rrd))
    if existing_rrd not in incomplete_replicas_rrd and filetime < older_than:
        # Delete pngs and rrd files
        subprocess.call("rm -f %s" % existing_rrd, shell=True) 


# Copying rrds to the /var/www location
subprocess.call("rm -rf /var/www/html/dynamo/dynamo/dealermon/monitoring/T*", shell=True)
subprocess.call("cp -r /var/spool/dynamo/track_transfers/T* /var/www/html/dynamo/dynamo/dealermon/monitoring/", shell=True)
subprocess.call("rm -f /var/www/html/dynamo/dynamo/dealermon/monitoring/total.rrd", shell=True)
subprocess.call("cp /var/spool/dynamo/track_transfers/total.rrd /var/www/html/dynamo/dynamo/dealermon/monitoring/", shell=True)
subprocess.call("rm -f /var/www/html/dynamo/dynamo/dealermon/monitoring/overview.txt", shell=True)
subprocess.call("cp /var/spool/dynamo/track_transfers/overview.txt /var/www/html/dynamo/dynamo/dealermon/monitoring/", shell=True)

